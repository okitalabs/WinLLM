# WindowsでLLM
# llama-cpp-pytho直起動編

Dockerを使わずに、llama-cpp-pythonをWSLから直接起動するための手順。  
インストール時にコンパイルするため開発環境が必要。  


> 注意： インストールで`pip install llama_cpp_python==0.2.55`とバージョンを指定しているのは、`0.2.56`でEmbeddingsが動作しないバグため、前のバージョンを指定している。最新版でバグが修正されたらバージョン指定は不要。


## CPU版
### llama-cpp-pythonのインストール
```bash
sudo apt update
sudo apt install -y libopenblas-dev ninja-build build-essential pkg-config

pip install --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette pydantic-settings starlette-context setuptools wheel

CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" pip install llama_cpp_python==0.2.55 --verbose
```

### llama_cpp.serverの起動
```bash
python -m llama_cpp.server --model ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf --chat_format llama-2 --port 8080 --host 0.0.0.0
```

<hr>

## GPU版
GPUを使用する場合、CUDAの環境も必要。

### llama-cpp-pythonのインストール
```bash
sudo apt-get update

sudo apt-get install -y git build-essential \
    python3 python3-pip gcc wget \
    ocl-icd-opencl-dev opencl-headers clinfo \
    libclblast-dev libopenblas-dev

sudo mkdir -p /etc/OpenCL/vendors

sudo echo "libnvidia-opencl.so.1" > sudo /etc/OpenCL/vendors/nvidia.icd

pip install --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette pydantic-settings starlette-context setuptools wheel


set "CMAKE_ARGS=-DLLAMA_OPENBLAS=on"
set "FORCE_CMAKE=1"
pip install llama-cpp-python --no-cache-dir
```

### 起動
llama_cpp.serverの起動
GPUを使用する場合`--n_gpu_layers`を指定する。`-1`の場合、全てGPUメモリを使う。`30`で、7GBくらいをGPUメモリで使用し、残りはCPUメモリを使用する。
```bash
python -m llama_cpp.server --model ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf --chat_format llama-2 --port 8080 --host 0.0.0.0 --n_gpu_layers -1
```

<hr>

LLM実行委員会
