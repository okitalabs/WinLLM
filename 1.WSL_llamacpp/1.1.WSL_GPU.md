# WindowsでLLM
# WSL GPU編

WSLでNVIDIA GPUを使うためのインストール手順。

## はじめに
WSLでGPUを使うための前提条件が多い。以下の条件を満たすかをチェックする。

参考： [CUDA on WSL User Guide](https://docs.nvidia.com/cuda/wsl-user-guide/index.html)

### H/W構成
| | |
|:----|:----|
|機種名|OMEN X by HP Laptop 17-ap0xx|
|CPU|Core i7-7820HK|
|Memory|32GB|
|SSD|512GB|
|GPU|NVIDIA GTX-1080 Laptop/8GB|
|OS|Windows10 22H2|


### WindowsのGPUドライバの確認
GPUドライバのバージョンが545以降。  
バージョンが古い場合、[NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)からダウンロードして最新版にアップデートする。
> 確認方法  
> デスクトップで右クリック　NVIDIAコントロールパネル  
> または  
> アプリと機能画面　NVIDIAグラフィックドライバー  


### Windows10のバージョン
`21H2`以降であること。

### WSLのバージョン
`WSL2`以降であること。  
WSLのインストール後に確認する。

<hr>

## WSLのインストール
WSLに`Ubuntu-22.04`をインストールする。  
コマンドプロンプトから以下を実行する。
```
C:\Users\me> wsl --install -d Ubuntu-22.04
```

> 以下のエラーが発生する場合、Microsoft Storeから`Ubuntu 22.04.3 LTS`をインストールしてみる。
> ```
>致命的なエラーです。  
>Error code: Wsl/InstallDistro/E_UNEXPECTED
> ```

インストール可能なディストリビューション一覧
```
C:\Users\me> wsl --list --online
インストールできる有効なディストリビューションの一覧を次に示します。
'wsl.exe --install <Distro>' を使用してインストールします。

NAME                                   FRIENDLY NAME
Ubuntu                                 Ubuntu
Debian                                 Debian GNU/Linux
kali-linux                             Kali Linux Rolling
Ubuntu-18.04                           Ubuntu 18.04 LTS
Ubuntu-20.04                           Ubuntu 20.04 LTS
Ubuntu-22.04                           Ubuntu 22.04 LTS
OracleLinux_7_9                        Oracle Linux 7.9
OracleLinux_8_7                        Oracle Linux 8.7
OracleLinux_9_1                        Oracle Linux 9.1
openSUSE-Leap-15.5                     openSUSE Leap 15.5
SUSE-Linux-Enterprise-Server-15-SP4    SUSE Linux Enterprise Server 15 SP4
SUSE-Linux-Enterprise-15-SP5           SUSE Linux Enterprise 15 SP5
openSUSE-Tumbleweed                    openSUSE Tumbleweed
```

インストール後、Ubuntu Shellを起動すると、ユーザ登録になるので、初期
アカウントを入力する。
```
Installing, this may take a few minutes...
Please create a default UNIX user account. The username does not need to match your Windows username.
For more information visit: https://aka.ms/wslusers
Enter new UNIX username: okita
New password:
Retype new password:
```

WSLバージョンの確認  
VERSIONが`2`であること。
```
C:\Users\me>wsl -l -v
  NAME            STATE           VERSION
* Ubuntu-22.04    Running         2
```

カーネルバージョンの確認  
5.xxであること。
```
C:\Users\me>wsl -d Ubuntu-22.04 uname -r
5.15.146.1-microsoft-standard-WSL2
```

> WSL1だった場合、WSL2に変換する。  
> [WSL1 から WSL2への移行](https://qiita.com/ryamamoto0406/items/d43f19e8821d3e28f1cb)

<hr>

## CUDA/cuDNNのインストール

NVIDIA CUDA パッケージレポジトリを，Ubuntu システムに追加
```
sudo wget -O /etc/apt/preferences.d/cuda-repository-pin-600 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin

sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub

sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /"

sudo apt -y update
```
> 以下のエラーが出た場合、Windowsの時刻が正しくない可能性が高い。時刻を修正する。  
> `E: Release file for http://security.ubuntu.com/ubuntu/dists/jammy-security/InRelease is not valid yet (invalid for another 8h 17min 23s). Updates for this repository will not be applied.`


パッケージのインストール
```
sudo apt -y install cuda-12-4 ## CUDA
sudo apt -y install cudnn9-cuda-12 libcudnn9-dev-cuda-12 ## cuDNN
```

`/usr/local/cuda`にインストールされるので、環境変数をセットする。
```
export CUDA_PATH=/usr/local/cuda-12
echo 'export CUDA_PATH=/usr/local/cuda-12' >> ${HOME}/.bashrc
export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:${LD_LIBRARY_PATH}
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:${LD_LIBRARY_PATH}' >> ${HOME}/.bashrc
export PATH=/usr/local/cuda-12/bin:${PATH}
echo 'export PATH=/usr/local/cuda-12/bin:${PATH}' >> ${HOME}/.bashrc
```
再読み込み  
`source ~/.bashrc`

> インストール可能なパッケージバージョンの確認
> ```
> apt-cache search cuda ## CUDAパッケージ一覧
> apt-cache search cuda ## CUDAパッケージ一覧
>```

GPU認識確認  
`GPU Memory`が`N/A`になる... (˘-ω-˘ ).｡oஇ
```
$ nvidia-smi
Mon Mar 18 00:06:13 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 551.76         CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1080        On  |   00000000:01:00.0 Off |                  N/A |
| N/A   44C    P8              9W /  160W |     468MiB /   8192MiB |      4%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A        52      G   /Xwayland                                   N/A      |
+-----------------------------------------------------------------------------------------+
```

<hr>

## Python環境の構築
conda-forgeを使用した`conda`環境を構築する。
```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh

## インストール処理
  :
You can undo this by running `conda init --reverse $SHELL`? [yes|no]
[no] >>> yes
```

Python仮想環境の作成  
`llm-3.10`を`MKL`対応環境として構築。
```
conda create -n llm-3.10 python=3.10 blas=*=*mkl
```

Activate
python仮想環境は複数作成ができ、Activateして使用したい環境を選択する。 選択すると、プロンプトが環境名に変わる。  
`.bashrc`に書いておくとグッド（気付かないでbaseにインストールしちゃったりするので）
```
conda activate llm-3.10

(llm3.10) hostname:~$  ## プロンプトが変わる
```

MKL対応ライブラリはあらかじめ入れておく。
```
conda install -c intel numpy scikit-learn scipy ## MLK対応
```


## Pytorch環境の構築
今回は不要  
[Pytorch](https://pytorch.org/)のサイトで最新版のインストールコマンドを確認。

```
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
```

<hr>

## GPU対応の`llama-cpp-python`のインストール

Dockerだと面倒なので、WSL直起動が楽。　　
インストール時にコンパイルするため開発も入れる必要がある。
```
sudo apt-get update

sudo apt-get install -y git build-essential \
    python3 python3-pip gcc wget \
    ocl-icd-opencl-dev opencl-headers clinfo \
    libclblast-dev libopenblas-dev

sudo mkdir -p /etc/OpenCL/vendors

sudo echo "libnvidia-opencl.so.1" > sudo /etc/OpenCL/vendors/nvidia.icd

CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python==0.2.55 --verbose
```

### LLMモデル起動（Elyza）
llama-cpp-pythonサーバをElyzaモデルで起動してみる。  
モデルは`~/llm/model`に配置する。

```
mkdir -p llm/model ## モデル置き場

wget https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf/resolve/main/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf ## モデルのダウンロード
```

モデルの起動
```
python -m llama_cpp.server --model ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf --chat_format llama-2 --port 8080 --host 0.0.0.0 --n_gpu_layers -1
```

動作確認
```
curl -s -XPOST -H 'Content-Type: application/json' \
    localhost:8080/v1/chat/completions \
    -d '{"messages": [{"role": "user", "content": "東京の名所を5つ教えてください。その理由も述べてください。"}]}'
```
サーバのレスポンス  
生成では、
36.24Token/sくらい。
```
llama_print_timings:        load time =     395.56 ms
llama_print_timings:      sample time =     229.33 ms /   368 runs   (    0.62 ms per token,  1604.70 tokens per second)
llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_print_timings:        eval time =   10154.07 ms /   368 runs   (   27.59 ms per token,    36.24 tokens per second)
llama_print_timings:       total time =   11550.37 ms /   369 tokens
```

### 起動時のメッセージ
```
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1080, compute capability 6.1, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = ELYZA-japanese-Llama-2-7b-fast-instruct
llama_model_loader: - kv   2:      general.source.huggingface.repository str              = elyza/ELYZA-japanese-Llama-2-7b-fast-...
llama_model_loader: - kv   3:                   llama.tensor_data_layout str              = Meta AI original pth
llama_model_loader: - kv   4:                       llama.context_length u32              = 4096
llama_model_loader: - kv   5:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   6:                          llama.block_count u32              = 32
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   8:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,45043]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,45043]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,45043]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: mismatch in special tokens definition ( 304/45043 vs 264/45043 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 45043
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.85 B
llm_load_print_meta: model size       = 3.87 GiB (4.85 BPW)
llm_load_print_meta: general.name     = ELYZA-japanese-Llama-2-7b-fast-instruct
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    98.97 MiB
llm_load_tensors:      CUDA0 buffer size =  3862.73 MiB
warning: failed to mlock 104783872-byte buffer (after previously locking 0 bytes): Cannot allocate memory
Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).
................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2
AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 |
Model metadata: {'general.file_type': '15', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'ELYZA-japanese-Llama-2-7b-fast-instruct', 'general.source.huggingface.repository': 'elyza/ELYZA-japanese-Llama-2-7b-fast-instruct', 'llama.embedding_length': '4096', 'llama.tensor_data_layout': 'Meta AI original pth', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama'}
INFO:     Started server process [843]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
```
GPUの認識  
`Device 0: NVIDIA GeForce GTX 1080`を認識。
```
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1080, compute capability 6.1, VMM: yes
```
使用メモリ量  
33layer全てをGPUへオフロード。  
CPU 98.97 MiB、GPU 3862.73 MiBを使用。
```
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    98.97 MiB
llm_load_tensors:      CUDA0 buffer size =  3862.73 MiB
```
入出力の文字数が多くなると使用メモリが増大するため、2割くらい多く見積もっておく（4.5GBくらい）。  


<hr>

## Docker環境の構築
Dockerは色々なLLMパッケージの導入で使うことが多いので入れておいた方が良い。
また、aptなどでLinuxライブラリを入れ必要がある場合、環境がグチャグチャになるため、Dockerで試験した方が良い。

### Dockerのインストール
```
sudo apt update
sudo apt install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install docker-ce docker-ce-cli containerd.io
```
ユーザhogeにdocker権限付与
```
sudo usermod -aG docker okita
newgrp docker
sudo service docker restart
```


### NVIDIA Container Toolkitのインストール
DockerでGPUを使う場合必要。
```
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
    sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
    && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
        sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
        sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt update

sudo apt install -y nvidia-container-toolkit

sudo systemctl restart docker
```

<hr>

## GPU対応Docker版llama-cpp-pythonの構築

### cuda_simpleのイメージ作成
GitHubからリポジトリをコピー。
```
git clone https://github.com/abetlen/llama-cpp-python
```

Dockerイメージのビルド。`cuda_simple`を使用する。
```
 cd llama-cpp-python/docker/cuda_simple/
 docker build -t cuda_simple .
```

イメージの確認。`cuda_simple`が作成される。
```
docker images

REPOSITORY    TAG       IMAGE ID       CREATED        SIZE
cuda_simple   latest    817ac8810e9f   22 hours ago   8.04GB
```

### コンテナの起動
GPUを使用する場合、`--gpus`オプションを指定する。
```
docker run -d --gpus all --cap-add SYS_RESOURCE -p 8080:8080 -e USE_MLOCK=0 -e MODEL=/home/okita/llm/model -v  /home/okita/llm/model:/home/model --name llm -t cuda_simple /bin/bash
```

### コンテナ内でGPUが使えるか確認
コンテナに入り`nvidia-smi`を実行する。
```
docker exec -it llm /bin/bash

##　コンテナ内
nvidia-smi
Mon Mar 18 14:51:56 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.60.01              Driver Version: 551.76         CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1080        On  |   00000000:01:00.0 Off |                  N/A |
| N/A   29C    P8             11W /  160W |     220MiB /   8192MiB |      2%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```


### llama-cpp.serverの起動
起動は、`python3`で、GPUを使用する場合`--n_gpu_layers -1`オプションを付ける。
```
docker exec -it llm /bin/bash

##　コンテナ内
cd /home/model

python3 -m llama_cpp.server --model ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf --chat_format llama-2 --port 8080 --host 0.0.0.0 --n_gpu_layers -1
```

動作確認
```
curl -s -XPOST -H 'Content-Type: application/json' \
    localhost:8080/v1/chat/completions \
    -d '{"messages": [{"role": "user", "content": "東京の名所を5つ教えてください。その理由も述べてください。"}]}'
```
サーバのレスポンス  
生成では、
34.63Token/sくらい。
```
llama_print_timings:        load time =     418.02 ms
llama_print_timings:      sample time =     147.38 ms /   246 runs   (    0.60 ms per token,  1669.17 tokens per second)
llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_print_timings:        eval time =    7102.98 ms /   246 runs   (   28.87 ms per token,    34.63 tokens per second)
llama_print_timings:       total time =    8262.07 ms /   247 tokens
```
WSL直起動とDocker起動の性能差が2Token/sくらいある。
- WSL直起動: 36.24Token/s
- Docker起動: 34.63Token/s
<hr>

## Tips
- WSL環境削除  
    `wsl --unregister Ubuntu-22.04`
 
 
 <hr>

LLM実行委員会