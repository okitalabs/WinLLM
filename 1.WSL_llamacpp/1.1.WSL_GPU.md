# WindowsでLLM
# WSL GPU編

WSLでNVIDIA GPUを使うためのインストール手順。

## はじめに
WSLでGPUを使うための前提条件が多い。以下の条件を満たすかをチェックする。

参考： [CUDA on WSL User Guide](https://docs.nvidia.com/cuda/wsl-user-guide/index.html)

### H/W構成
| | |
|:----|:----|
|機種名|OMEN X by HP Laptop 17-ap0xx|
|CPU|Core i7-7820HK|
|Memory|32GB|
|SSD|512GB|
|GPU|NVIDIA GTX-1080/8GB|
|OS|Windows10 22H2|


### WindowsのGPUドライバの確認
GPUドライバのバージョンが545以降。  
バージョンが古い場合、[NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)からダウンロードして最新版にアップデートする。
> 確認方法  
> デスクトップで右クリック　NVIDIAコントロールパネル  
> または  
> アプリと機能画面　NVIDIAグラフィックドライバー  


### Windows10のバージョン
`21H2`以降であること。

### WSLのバージョン
`WSL2`以降であること。  
WSLのインストール時に指定する。

<hr>

## WSLのインストール
WSLに`Ubuntu-22.04`をインストールする。  
コマンドプロンプトから以下を実行する。
```
C:\Users\me> wsl --install -d Ubuntu-22.04
```

> 以下のエラーが発生する場合、Microsoft Storeから`Ubuntu 22.04.3 LTS`をインストールしてみる。
> ```
>致命的なエラーです。  
>Error code: Wsl/InstallDistro/E_UNEXPECTED
> ```

インストール可能なディストリビューション一覧
```
C:\Users\me> wsl --list --online
インストールできる有効なディストリビューションの一覧を次に示します。
'wsl.exe --install <Distro>' を使用してインストールします。

NAME                                   FRIENDLY NAME
Ubuntu                                 Ubuntu
Debian                                 Debian GNU/Linux
kali-linux                             Kali Linux Rolling
Ubuntu-18.04                           Ubuntu 18.04 LTS
Ubuntu-20.04                           Ubuntu 20.04 LTS
Ubuntu-22.04                           Ubuntu 22.04 LTS
OracleLinux_7_9                        Oracle Linux 7.9
OracleLinux_8_7                        Oracle Linux 8.7
OracleLinux_9_1                        Oracle Linux 9.1
openSUSE-Leap-15.5                     openSUSE Leap 15.5
SUSE-Linux-Enterprise-Server-15-SP4    SUSE Linux Enterprise Server 15 SP4
SUSE-Linux-Enterprise-15-SP5           SUSE Linux Enterprise 15 SP5
openSUSE-Tumbleweed                    openSUSE Tumbleweed
```

インストール後、Ubuntu Shellを起動すると、ユーザ登録になるので、初期
アカウントを入力する。
```
Installing, this may take a few minutes...
Please create a default UNIX user account. The username does not need to match your Windows username.
For more information visit: https://aka.ms/wslusers
Enter new UNIX username: me
New password:
Retype new password:
```

WSLバージョンの確認  
VERSIONが`2`であること。
```
C:\Users\me>wsl -l -v
  NAME            STATE           VERSION
* Ubuntu-22.04    Running         2
```

カーネルバージョンの確認  
5.xxであること。
```
C:\Users\me>wsl -d Ubuntu-22.04 uname -r
5.15.146.1-microsoft-standard-WSL2
```

<hr>

## CUDA/cuDNNのインストール

NVIDIA CUDA パッケージレポジトリを，Ubuntu システムに追加
```
sudo wget -O /etc/apt/preferences.d/cuda-repository-pin-600 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin

sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub

sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /"

sudo apt -y update
```
> 以下のエラーが出た場合、Windowsの時刻が正しくない可能性が高い。時刻を修正する。  
> `E: Release file for http://security.ubuntu.com/ubuntu/dists/jammy-security/InRelease is not valid yet (invalid for another 8h 17min 23s). Updates for this repository will not be applied.`


パッケージのインストール
```
sudo apt -y install cuda-12-4 ## CUDA
sudo apt -y install cudnn9-cuda-12 libcudnn9-dev-cuda-12 ## cuDNN
```

`/usr/local/cuda`にインストールされるので、環境変数をセットする。
```
export CUDA_PATH=/usr/local/cuda-12
echo 'export CUDA_PATH=/usr/local/cuda-12' >> ${HOME}/.bashrc
export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:${LD_LIBRARY_PATH}
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:${LD_LIBRARY_PATH}' >> ${HOME}/.bashrc
export PATH=/usr/local/cuda-12/bin:${PATH}
echo 'export PATH=/usr/local/cuda-12/bin:${PATH}' >> ${HOME}/.bashrc
```
再読み込み  
`source ~/.bashrc`

> インストール可能なパッケージバージョンの確認
> ```
> apt-cache search cuda ## CUDAパッケージ一覧
> apt-cache search cuda ## CUDAパッケージ一覧
>```

GPU認識確認  
`GPU Memory`が`N/A`になる... (˘-ω-˘ ).｡oஇ
```
$ nvidia-smi
Mon Mar 18 00:06:13 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 551.76         CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1080        On  |   00000000:01:00.0 Off |                  N/A |
| N/A   44C    P8              9W /  160W |     468MiB /   8192MiB |      4%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A        52      G   /Xwayland                                   N/A      |
+-----------------------------------------------------------------------------------------+
```

<hr>

## Python環境の構築
conda-forgeを使用した`conda`環境を構築する。
```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

bash Miniconda3-latest-Linux-x86_64.sh

## インストール処理
  :
You can undo this by running `conda init --reverse $SHELL`? [yes|no]
[no] >>> yes
```

Python仮想環境の作成  
`llm-3.10`を`MKL`対応環境として構築。
```
conda create -n llm-3.10 python=3.10 blas=*=*mkl
```

Activate
python仮想環境は複数作成ができ、Activateして使用したい環境を選択する。 選択すると、プロンプトが環境名に変わる。  
`.bashrc`に書いておくとグッド（気付かないでbaseにインストールしちゃったりするので）
```
conda activate llm-3.10

(llm3.10) hostname:~$  ## プロンプトが変わる
```

MKL対応ライブラリはあらかじめ入れておく。
```
conda install -c intel numpy scikit-learn scipy ## MLK対応
```


## Pytorch環境の構築
今回は不要  
[Pytorch](https://pytorch.org/)のサイトで最新版のインストールコマンドを確認。

```
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
```

<hr>

## GPU対応の`llama-cpp-python`のインストール

Dockerだと面倒なので、WSL直起動が楽。　　
インストール時にコンパイルするため開発も入れる必要がある。
```
sudo apt-get update

sudo apt-get install -y git build-essential \
    python3 python3-pip gcc wget \
    ocl-icd-opencl-dev opencl-headers clinfo \
    libclblast-dev libopenblas-dev

sudo mkdir -p /etc/OpenCL/vendors

sudo echo "libnvidia-opencl.so.1" > sudo /etc/OpenCL/vendors/nvidia.icd

CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python==0.2.55 --verbose
```

### LLMモデル起動（Elyza）
llama-cpp-pythonサーバをElyzaモデルで起動してみる。  
モデルは`~/llm/model`に配置する。

```
mkdir -p llm/model ## モデル置き場

wget https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf/resolve/main/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf ## モデルのダウンロード
```

モデルの起動
```
python -m llama_cpp.server --model ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf --chat_format llama-2 --port 8080 --host 0.0.0.0 --n_gpu_layers -1
```


## Docker環境の構築
Dockerは色々なLLMパッケージの導入で使うことが多いので入れておいた方が良い。

### Dockerのインストール
```
sudo apt update
sudo apt install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install docker-ce docker-ce-cli containerd.io
```
ユーザhogeにdocker権限付与
```
sudo usermod -aG docker me
newgrp docker
sudo service docker restart
```


### NVIDIA Container Toolkitのインストール
DockerでGPUを使う場合必要。
```
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
    sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
    && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
        sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
        sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt update

sudo apt install -y nvidia-container-toolkit

sudo systemctl restart docker
```

<hr>

## GPU対応Docker版llama-cpp-python

### llama-cpp-pythonのコンテナ作成
GitHubからリポジトリをコピー。
```
git clone https://github.com/abetlen/llama-cpp-python
```

```
 cd llama-cpp-python/docker/cuda_simple/
 docker build -t cuda_simple .
```

> llama-cpp-pythonの最新版0.2.56で起きるため、Dockerfileのpip installで0.2.55を入れるようにDockerfileを修正する。  
> `RUN CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python==.2.55`

### モデルのダウンロード
ELYZAのモデルを`~/llm/model/`に落とすことにする。
```
mkdir -p ~/llm/model
cd ~/llm/model
wget https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf/resolve/main/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf
```

### LLMの起動
llama_cpp.serverの起動 GPUを使用する場合--n_gpu_layersを指定する。-1の場合、全てGPUメモリを使う。30で、7GBくらいをGPUメモリで使用し、残りはCPUメモリを使用する。
```
python -m llama_cpp.server --model ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf --chat_format llama-2 --port 8080 --host 0.0.0.0 --n_gpu_layers -1
```

### 動作確認
```
curl -s -XPOST -H 'Content-Type: application/json' \
    localhost:8080/v1/chat/completions \
    -d '{"messages": [{"role": "user", "content": "東京の名所は？"}]}'
```
サーバのレスポンス  
生成では、
37.64Token/sくらい。
```
llama_print_timings:        load time =     517.03 ms
llama_print_timings:      sample time =      80.71 ms /   133 runs   (    0.61 ms per token,  1647.85 tokens per second)
llama_print_timings: prompt eval time =     516.98 ms /    17 tokens (   30.41 ms per token,    32.88 tokens per second)
llama_print_timings:        eval time =    3506.68 ms /   132 runs   (   26.57 ms per token,    37.64 tokens per second)
llama_print_timings:       total time =    4554.21 ms /   149 tokens
```


<hr>

## Tips
- WSL環境削除  
    `wsl --unregister Ubuntu-22.04`
 
 
 <hr>

LLM実行委員会